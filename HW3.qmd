---
title: "HW3"
format: 
  html:
    embed-resources: true
---

## Ece Naz Çalkınsın 22003342 Homework 3

```{r}
library(magrittr)
library(tidyverse)
library(ISLR2)
```

I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response.

```{r}
set.seed(777)
X <- rnorm(100)
Y <- 7*X + rnorm(100)

```

I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 +β1X +β2X2 +β3X3 +ε.

```{r}
linear_model <- lm(Y ~ X)

cubic_model <- lm(Y ~ poly(X, 3))
```

\(a\) Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ε. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

**If the true relationship between X and Y is linear, it is expected that linear regression model will provide a better fit than the cubic model. Hence, RSS for the linear model would be expected to be lower than the cubic regression model.**

```{r}
linear_predicted <- predict(linear_model)
linear_residuals <- Y - linear_predicted  
linear_RSS <- sum(linear_residuals^2) 
linear_RSS

cubic_predicted <- predict(cubic_model)  
cubic_residuals <- Y - cubic_predicted  
cubic_RSS <- sum(cubic_residuals^2)  
cubic_RSS
```

**It seems as tough RSS for cubic regression is lower than the linear regression, altough the reverse was expected because of the assumption that true relationship is linear.**

\(b\) Answer (a) using test rather than training RSS.

```{r}
anova(linear_model, cubic_model)
```

\(c\) Suppose that the true relationship between X and Y is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

**If the true relationship between X and Y is non-linear, it is expected that non-linear regression model will provide a better fit than the linear model. Hence, RSS for the cubic model would be expected to be lower than the linear regression model.**

```{r}
linear_predicted <- predict(linear_model)
linear_residuals <- Y - linear_predicted  
linear_RSS <- sum(linear_residuals^2) 
linear_RSS

cubic_predicted <- predict(cubic_model)  
cubic_residuals <- Y - cubic_predicted  
cubic_RSS <- sum(cubic_residuals^2)  
cubic_RSS
```

**The result is as expected, lower RSS for cubic model.**

\(d\) Answer (c) using test rather than training RSS.

```{r}
anova(linear_model, cubic_model)
```

**8.** This question involves the use of simple linear regression on the Auto data set.

**(a)** Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output. For example:

```{r}
fit <- lm(mpg ~ horsepower, data = Auto)
summary(fit)
```

i\. Is there a relationship between the predictor and the response?

**Yes, there is a relationship because p value is less than 0.05.**

ii\. How strong is the relationship between the predictor and the response?

**It is a very strong relationship because it has \*\*\* (three stars) next to it, meaning that p value is between 0 and 0.001, which is very less than 0.05.The less the p-value is, there is more significance.**

iii\. Is the relationship between the predictor and the response positive or negative?

**The relationship is negative because the estimator is less than zero.**

iv\. What is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals?

```{r}
predict(fit, newdata = data.frame(horsepower = 98), interval = "prediction", level = 0.95)

predict(fit, newdata = data.frame(horsepower = 98), interval = "confidence", level = 0.95)

```

**The predicted mpg associated with a horsepower of 98 is 24.46708.**

**The prediction interval suggests that we are 95% confident that the mpg at horsepower of 98 is between 14.8094 and 34.12476.**

**The confidence interval suggests that we are 95% confident that the 'true expected value' of mpg at horsepower of 98 is between 23.97308 to 24.96108.**

\(b\) Plot the response and the predictor. Use the abline() function to display the least squares regression line.

```{r}
plot(Auto$horsepower,Auto$mpg, col = "magenta")
abline(abline(fit, col = "blue"))
```

\(c\) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

```{r}
par(mfrow = c(2,2))
plot(fit)
```

**A U-shaped or curved pattern in the residuals might suggest nonlinearity in the model, indicating that a linear regression model may not be the best fit for the data.**

**If the residuals closely follow a normal distribution, the points on the Q-Q plot should fall approximately along a straight line at a 45-degree angle. Hence, the second graph demonstrates that the residuals follow a normal distribution.**

**Ideally, the points on the plot should form a horizontal band or a roughly constant spread around the central line, which means that the the variability in errors remains consistent.**

**Finally, the fourth graph indicates that there are no significant data points or outliers that are affecting the model**

8.  We will now perform cross-validation on a simulated data set.

```{=html}
<!-- -->
```
(a) Generate a simulated data set as follows:

```{r}
set.seed(1)
x <- rnorm(100)
y <- x - 2 * x^2 + rnorm(100)
```

In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

**n is 100 and p is 1. n is the number of data points and p is number of predictor, which in this case only 'x'. The model used to generate data is:**

$$
y = x - 2x^2 + \epsilon
$$

(b) Create a scatterplot of X against Y . Comment on what you find.

```{r}
plot(x,y)
```

**The plot results as a curve shape as expected because the biggest power i the equation is 2, which is squared and it results in a curved shape.**

(c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:

```{=html}
<!-- -->
```
i.  Y = β0 + β1X + ε
ii. Y =β0 +β1X+β2X2 +ε
iii. Y =β0 +β1X+β2X2 +β3X3 +ε
iv. Y =β0 +β1X+β2X2 +β3X3 +β4X4 +ε. Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y.

```{r}
library(boot)
set.seed(100)
data <- data.frame(x, y)
fit1 <- glm(y ~ x)
cv.glm(data, fit1)$delta[1]
```

```{r}
fit2 <- glm(y ~ poly(x, 2))
cv.glm(data, fit2)$delta[1]
```

```{r}
fit3 <- glm(y ~ poly(x, 3))
cv.glm(data, fit3)$delta[1]
```

```{r}
fit4 <- glm(y ~ poly(x, 4))
cv.glm(data, fit4)$delta[1]
```

(d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?

```{r}
library(boot)
set.seed(90)
data <- data.frame(x, y)
fit1 <- glm(y ~ x)
cv.glm(data, fit1)$delta[1]
```

```{r}
fit2 <- glm(y ~ poly(x, 2))
cv.glm(data, fit2)$delta[1]
```

```{r}
fit3 <- glm(y ~ poly(x, 3))
cv.glm(data, fit3)$delta[1]
```

```{r}
fit4 <- glm(y ~ poly(x, 4))
cv.glm(data, fit4)$delta[1]
```

**Yes, they are the same because in LOOCV, you'll obtain the same results regardless of the random seed because the process itself is not based on randomization. Each data point is treated the same way in each run, leading to a deterministic outcome.**

(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.

**The second one had the smallest LOOCV error and that was what I expected because the optimal model is with a ploynomial degree 2.**

(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?

```{r}
summary(fit4)
```

**Looking at the p-values, it is seen that the first and the second models have p-values less than 0.05 and values very close to zero. This means that they are statistically significant. The lowest LOOCV error belongs to the second model, which aligns with statistical significance. However, the first model has a high LOOCV error, which does not agree with this statistical significance.**
